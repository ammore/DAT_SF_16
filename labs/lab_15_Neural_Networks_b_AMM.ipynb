{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GA Data Science (DAT16) - Lab 15\n",
    "## Deep Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Authors: Yann N. Dauphin, Vlad Niculae, Gabriel Synnaeve\n",
    "# License: BSD\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.ndimage import convolve\n",
    "from sklearn import linear_model, datasets, metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nudge_dataset(X, Y):\n",
    "    \"\"\"\n",
    "    This produces a dataset 5 times bigger than the original one,\n",
    "    by moving the 8x8 images in X around by 1px to left, right, down, up\n",
    "    \"\"\"\n",
    "    direction_vectors = [\n",
    "        [[0, 1, 0],\n",
    "         [0, 0, 0],\n",
    "         [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "         [1, 0, 0],\n",
    "         [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "         [0, 0, 1],\n",
    "         [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "         [0, 0, 0],\n",
    "         [0, 1, 0]]]\n",
    "\n",
    "    shift = lambda x, w: convolve(x.reshape((8, 8)), mode='constant',\n",
    "                                  weights=w).ravel()\n",
    "    X = np.concatenate([X] +\n",
    "                       [np.apply_along_axis(shift, 1, X, vector)\n",
    "                        for vector in direction_vectors])\n",
    "    Y = np.concatenate([Y for _ in range(5)], axis=0)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "digits = datasets.load_digits()\n",
    "X = np.asarray(digits.data, 'float32')\n",
    "X, Y = nudge_dataset(X, digits.target)\n",
    "X = (X - np.min(X, 0)) / (np.max(X, 0) + 0.0001)  # 0-1 scaling\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Models we will use\n",
    "logistic = linear_model.LogisticRegression()\n",
    "rbm = BernoulliRBM(random_state=0, verbose=True)\n",
    "\n",
    "classifier = Pipeline(steps=[('rbm', rbm), ('logistic', logistic)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Hyper-parameters. These were set by cross-validation,using a GridSearchCV. Here we are not performing cross-validation to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rbm.learning_rate = 0.06\n",
    "rbm.n_iter = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More components tend to give better prediction performance, but larger fitting time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rbm.n_components = 100\n",
    "logistic.C = 6000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -25.39, time = 0.25s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -23.77, time = 0.38s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -22.94, time = 0.35s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -21.91, time = 0.33s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -21.69, time = 0.36s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -21.06, time = 0.36s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -20.89, time = 0.34s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -20.64, time = 0.36s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -20.36, time = 0.33s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -20.09, time = 0.32s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -20.08, time = 0.33s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -19.82, time = 0.34s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -19.64, time = 0.32s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -19.61, time = 0.34s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -19.57, time = 0.32s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -19.41, time = 0.35s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -19.30, time = 0.33s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -19.25, time = 0.33s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -19.27, time = 0.33s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -19.01, time = 0.32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('rbm', BernoulliRBM(batch_size=10, learning_rate=0.06, n_components=100, n_iter=20,\n",
       "       random_state=0, verbose=True)), ('logistic', LogisticRegression(C=6000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training RBM-Logistic Pipeline\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Logistic regression\n",
    "logistic_classifier = linear_model.LogisticRegression(C=100.0)\n",
    "logistic_classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "Logistic regression using RBM features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99       174\n",
      "          1       0.92      0.95      0.93       184\n",
      "          2       0.95      0.98      0.97       166\n",
      "          3       0.97      0.91      0.94       194\n",
      "          4       0.97      0.95      0.96       186\n",
      "          5       0.93      0.93      0.93       181\n",
      "          6       0.98      0.97      0.97       207\n",
      "          7       0.95      1.00      0.97       154\n",
      "          8       0.90      0.88      0.89       182\n",
      "          9       0.91      0.93      0.92       169\n",
      "\n",
      "avg / total       0.95      0.95      0.95      1797\n",
      "\n",
      "\n",
      "Logistic regression using raw pixel features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.94      0.89       174\n",
      "          1       0.57      0.55      0.56       184\n",
      "          2       0.72      0.85      0.78       166\n",
      "          3       0.76      0.74      0.75       194\n",
      "          4       0.85      0.82      0.84       186\n",
      "          5       0.74      0.75      0.75       181\n",
      "          6       0.93      0.88      0.91       207\n",
      "          7       0.86      0.90      0.88       154\n",
      "          8       0.68      0.55      0.61       182\n",
      "          9       0.71      0.74      0.72       169\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1797\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(\n",
    "        Y_test,\n",
    "        classifier.predict(X_test))))\n",
    "\n",
    "print(\"Logistic regression using raw pixel features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(\n",
    "        Y_test,\n",
    "        logistic_classifier.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num must be 0 <= num <= 20, not 21",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5a0dafabae95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     plt.imshow(comp.reshape((8, 8)), cmap=plt.cm.gray_r,\n\u001b[1;32m      5\u001b[0m                interpolation='nearest')\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36msubplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0mbyebye\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/matplotlib/figure.pyc\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/matplotlib/axes/_subplots.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m                     raise ValueError(\n\u001b[1;32m     63\u001b[0m                         \"num must be 0 <= num <= {maxn}, not {num}\".format(\n\u001b[0;32m---> 64\u001b[0;31m                             maxn=rows*cols, num=num))\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     warnings.warn(\"The use of 0 (which ends up being the \"\n",
      "\u001b[0;31mValueError\u001b[0m: num must be 0 <= num <= 20, not 21"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAADlCAYAAAB+mTpOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBht3kx1fX0UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107d92cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4.2, 4))\n",
    "for i, comp in enumerate(rbm.components_):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(comp.reshape((8, 8)), cmap=plt.cm.gray_r,\n",
    "               interpolation='nearest')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "plt.suptitle('20 components extracted by RBM', fontsize=16)\n",
    "plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
