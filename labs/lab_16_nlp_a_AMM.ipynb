{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence and Word Segmentation\n",
    "\n",
    "The first step in NLP is cutting text into its constituents. Namely, sentences and words. Let's see how well we can perform this task in base python.\n",
    "\n",
    "**DO NOT worry about writing efficient code.** We're just practicing NLP principles.\n",
    "\n",
    "It will useful to know the String methods! These are one of the most useful features of Python for text processing!\n",
    "\n",
    "https://docs.python.org/2/library/stdtypes.html#string-formatting-operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence segmentation\n",
    "\n",
    "Let's start with sentence segmentation. English typically end with a period, exclamation, or question mark. Let's start easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Keep a list of sentences, and a temp string with the current sentence. Append when you hit the right characters'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell for a HINT:\n",
    "import base64\n",
    "base64.decodestring('S2VlcCBhIGxpc3Qgb2Ygc2VudGVuY2VzLCBhbmQgYSB0ZW1wIHN0cmluZyB3aXRoIHRoZSBjdXJy\\nZW50IHNlbnRlbmNlLiBBcHBlbmQgd2hlbiB5b3UgaGl0IHRoZSByaWdodCBjaGFyYWN0ZXJz\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Keep a list of sentences, and a temp string with the current sentence. Append when you hit the right characters'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base64.decodestring('S2VlcCBhIGxpc3Qgb2Ygc2VudGVuY2VzLCBhbmQgYSB0ZW1wIHN0cmluZyB3aXRoIHRoZSBjdXJyZW50IHNlbnRlbmNlLiBBcHBlbmQgd2hlbiB5b3UgaGl0IHRoZSByaWdodCBjaGFyYWN0ZXJz\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Keep a list of sentences, and a temp string with the current sentence. Append when you hit the right characters'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base64.decodestring('S2VlcCBhIGxpc3Qgb2Ygc2VudGVuY2VzLCBhbmQgYSB0ZW1wIHN0cmluZyB3aXRoIHRoZSBjdXJyZW50IHNlbnRlbmNlLiBBcHBlbmQgd2hlbiB5b3UgaGl0IHRoZSByaWdodCBjaGFyYWN0ZXJz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining the input text and what the output should be.\n",
    "\n",
    "easy_text = \"I went to the zoo today. What do you think of that? I bet you hate it! Or maybe you don't\"\n",
    "easy_split_text = [\"I went to the zoo today.\",\n",
    "                   \"What do you think of that?\",\n",
    "                   \"I bet you hate it!\",\n",
    "                   \"Or maybe you don't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a function to split a string into sentences.\n",
    "# If you're familiar with regexes, feel free to use the re module\n",
    "\n",
    "def sentencer(text):\n",
    "    \n",
    "    sentances = []\n",
    "    delimiters = ['.','?','!']\n",
    "    current_sentance = ''\n",
    "    for letter in text:\n",
    "        if letter not in delimiters:\n",
    "            current_sentance += letter\n",
    "        else:\n",
    "            current_sentance += letter\n",
    "            sentances.append(current_sentance)\n",
    "            current_sentance = ''\n",
    "    if current_sentance != '':\n",
    "        sentances.append(current_sentance)\n",
    "    \n",
    "    # FILL IN CODE\n",
    "\n",
    "    return sentances\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations!\n"
     ]
    }
   ],
   "source": [
    "# test your function by running this cell\n",
    "\n",
    "if map(str.strip, sentencer(easy_text)) == easy_split_text:\n",
    "    print 'Congratulations!'\n",
    "else:\n",
    "    print 'Sorry, try again!'\n",
    "    print\n",
    "    print 'Your version:'\n",
    "    print sentencer(easy_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print easy_split_text\n",
    "### Sentence segmentation continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#justin's Method\n",
    "def sentencer(text):\n",
    "    sentences = []\n",
    "    sent = []\n",
    "    for char in text:\n",
    "        if sent == ' ':\n",
    "            continue\n",
    "        if char == '.':\n",
    "            sent.append(char)\n",
    "            sentences.append(sent)\n",
    "            sent = []\n",
    "        elif char == '?':\n",
    "            sent.append(char)\n",
    "            sentences.append(sent)\n",
    "            sent = []\n",
    "        elif char == '!':\n",
    "            sent.append(char)\n",
    "            sentences.append(sent)\n",
    "            sent = []\n",
    "        else:\n",
    "            sent.append(char)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "descriptor 'strip' requires a 'str' object but received a 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8f5b1b9f2b19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test your function by running this cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentencer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0measy_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0measy_split_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Congratulations!'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: descriptor 'strip' requires a 'str' object but received a 'list'"
     ]
    }
   ],
   "source": [
    "# test your function by running this cell\n",
    "\n",
    "if map(str.strip, sentencer(easy_text)) == easy_split_text:\n",
    "    print 'Congratulations!'\n",
    "else:\n",
    "    print 'Sorry, try again!'\n",
    "    print\n",
    "    print 'Your version:'\n",
    "    print sentencer(easy_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print easy_split_text\n",
    "### Sentence segmentation continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-3ae6557c1f2c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-3ae6557c1f2c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    What about cases where periods denote abbreviations? This time, try to do the same splits, but accommodate 'Dr.', 'Mrs.', 'Mr.', and 'Ms.'.\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "What about cases where periods denote abbreviations? This time, try to do the same splits, but accommodate 'Dr.', 'Mrs.', 'Mr.', and 'Ms.'.\n",
    "# defining the input text and what the output should be.\n",
    "\n",
    "med_text = \"My name is Dr. Lee. There is also a Mrs. Lee. Actually, there are tons! They're other people's wives.\"\n",
    "med_split_text = [\"My name is Dr. Lee.\",\n",
    "                  \"There is also a Mrs. Lee.\",\n",
    "                  \"Actually, there are tons!\",\n",
    "                  \"They're other people's wives.\"]\n",
    "# modify your last sentencer to account for these new patterns.\n",
    "\n",
    "\n",
    "def sentencer2(text):\n",
    "    '''take a string called `text` and return a list of strings, each containing a sentence'''\n",
    "    # FILL IN CODE\n",
    "\n",
    "    return sentences\n",
    "# test your function by running this cell\n",
    "\n",
    "if map(str.strip, sentencer2(med_text)) == med_split_text:\n",
    "    print 'Congratulations!'\n",
    "else:\n",
    "    print 'Sorry, try again!'\n",
    "    print\n",
    "    print 'Your version:'\n",
    "    print sentencer2(med_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print med_split_text\n",
    "### Take Home Exercise: sentence segmentation continued\n",
    "\n",
    "Abbreviations like 'a.k.a.' are harder to accommodate. This one is quite challenging, so you can skip it if you want to move on.\n",
    "# Run this cell for a HINT:\n",
    "import base64\n",
    "base64.decodestring('VHJ5IGFsbG93aW5nIHRoZSBzcGxpdHMgb24gdGhlIHBlcmlvZHMsIGJ1dCB0aGVuIHJlYXR0YWNo\\naW5nIGlmIHRoZSBuZXh0IHNlbnRlbmNlIGlzIG9ubHkgb25lIGNoYXJhY3RlciBsb25n\\n')\n",
    "# defining the input text and what the output should be.\n",
    "\n",
    "hard_text = \"I know an M.D., i.e. a doctor. Like Dr. Smith, a.k.a. Docsmith.\"\n",
    "hard_split_text = [\"I know an M.D., i.e. a doctor.\",\n",
    "                   \"Like Dr. Smith, a.k.a. Docsmith.\"]\n",
    "# take home exercise:\n",
    "# modify your last sentencer to account for these new patterns.\n",
    "\n",
    "def sentencer3(text):\n",
    "    '''take a string called `text` and return a list of strings, each containing a sentence'''\n",
    "    # FILL IN CODE\n",
    "\n",
    "    return sentences\n",
    "# test your function by running this cell\n",
    "\n",
    "if map(str.strip, sentencer3(hard_text)) == hard_split_text:\n",
    "    print 'Congratulations!'\n",
    "else:\n",
    "    print 'Sorry, try again!'\n",
    "    print\n",
    "    print 'Your version:'\n",
    "    print sentencer3(hard_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print hard_split_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence segmentation continued\n",
    "\n",
    "Sentence segmentation is harder than it seems! Let's take a look at how a modern system does it. [NLTK](http://www.nltk.org) is the most widely-used NLP library in Python. It [relies on a statistical language model](http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt) to determine when to split sentences. You'll notice that even this model can't handle our hard sentences.\n",
    "\n",
    "To get started, you have to download the right dataset. **DO NOT** download everything. It will take forever. When the download window pops up (probably behind your other windows, annoyingly) click on the 'Models' tab, choose the 'punkt' dataset, and just download that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did you read the instructions?\n"
     ]
    }
   ],
   "source": [
    "# download the Punkt Tokenizer Models.\n",
    "# DON'T DOWNLOAD EVERYTHING!\n",
    "# The download window will probably pop up behind your other windows.\n",
    "# uncomment the download command and comment out the print statement when you've understood these instructions.\n",
    "\n",
    "import nltk\n",
    "#nltk.download()\n",
    "print \"did you read the instructions?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource u'tokenizers/punkt/english.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/Users/AMM/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - u''\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f461db228ee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msent_detector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/english.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'tokenizers/punkt/english.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/Users/AMM/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - u''\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sent_detector.sentences_from_text(easy_text)\n",
    "print sent_detector.sentences_from_text(med_text)\n",
    "print sent_detector.sentences_from_text(hard_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word tokenization\n",
    "\n",
    "A more common task is to ignore sentences and just split text into words. We call this tokenization. Try your hand at this. This task is much easier now that you're familiar with all the string methods. right?? You should be able to write a fairly simple function that can tokenize all of our texts from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define our objective tokenizations. Note that we've removed some punctuation.\n",
    "easy_words = ['I', 'went', 'to', 'the', 'zoo', 'today',\n",
    "              'What', 'do', 'you', 'think', 'of', 'that',\n",
    "              'I', 'bet', 'you', 'hate', 'it',\n",
    "              'Or', 'maybe', 'you', \"don't\"]\n",
    "med_words = ['My', 'name', 'is', 'Dr', 'Lee',\n",
    "             'There', 'is', 'also', 'a', 'Mrs', 'Lee',\n",
    "             'Actually,', 'there', 'are', 'tons',\n",
    "             \"They're\", 'other', \"people's\", 'wives']\n",
    "hard_words = ['I', 'know', 'an', 'MD,', 'ie', 'a', 'doctor',\n",
    "              'Like', 'Dr', 'Smith,', 'aka', 'Docsmith']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a function to split a string into sentences.\n",
    "# If you're familiar with regexes, feel free to use the re module\n",
    "\n",
    "def tokenizer(text):\n",
    "    '''take a string called `text` and return a list of strings, each containing a WORD'''\n",
    "    # FILL IN CODE\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test your function by running this cell\n",
    "\n",
    "if tokenizer(easy_text) == easy_words:\n",
    "    print 'Congratulations!'\n",
    "else:\n",
    "    print 'Sorry, try again!'\n",
    "    print\n",
    "    print 'Your version:'\n",
    "    print tokenizer(easy_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print easy_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test your function by running this cell\n",
    "\n",
    "if tokenizer(med_text) == med_words:\n",
    "    print 'Congratulations!'\n",
    "else:\n",
    "    print 'Sorry, try again!'\n",
    "    print\n",
    "    print 'Your version:'\n",
    "    print tokenizer(med_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print med_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test your function by running this cell\n",
    "\n",
    "if tokenizer(hard_text) == hard_words:\n",
    "    print 'Congratulations!'\n",
    "else:\n",
    "    print 'Sorry, try again!'\n",
    "    print\n",
    "    print 'Your version:'\n",
    "    print tokenizer(hard_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print hard_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization continued\n",
    "\n",
    "Let's see how NLTK [tokenizes text into words](http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print word_tokenize(easy_text)\n",
    "print word_tokenize(med_text)\n",
    "print word_tokenize(hard_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It behaves a little differently, and sometimes erratically. Let's try a version based on pattern-matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print wordpunct_tokenize(easy_text)\n",
    "print wordpunct_tokenize(med_text)\n",
    "print wordpunct_tokenize(hard_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, there isn't really a right or wrong way to tokenize words. Sometimes punctuation provides valuable semantic content. Sometimes, you want to strip it all away.\n",
    "\n",
    "As a final thought, what do you suppose the following functions do? Go ahead and play with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for bigram in bigrams(word_tokenize(easy_text)):\n",
    "    print bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
